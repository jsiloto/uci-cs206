{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466cd22c",
   "metadata": {},
   "source": [
    "# 9.30 Gradient and Newton methods\n",
    "\n",
    "minimize\n",
    "f (x) = −\n",
    "P m\n",
    "i=1\n",
    "log(1 − a Ti x) −\n",
    "P n\n",
    "i=1\n",
    "log(1 − x 2 i ),\n",
    "\n",
    "a) Use the gradient method to solve the problem, using reasonable choices for the back-\n",
    "tracking parameters, and a stopping criterion of the form k∇f (x)k 2 ≤ η. Plot the\n",
    "objective function and step length versus iteration number. (Once you have deter-\n",
    "mined p ⋆ to high accuracy, you can also plot f − p ⋆ versus iteration.) Experiment\n",
    "with the backtracking parameters α and β to see their effect on the total number of\n",
    "iterations required. Carry these experiments out for several instances of the problem,\n",
    "of different sizes.\n",
    "\n",
    "(b) Repeat using Newton’s method, with stopping criterion based on the Newton decre-\n",
    "ment λ 2 . Look for quadratic convergence. You do not have to use an efficient method\n",
    "to compute the Newton step, as in exercise 9.27; you can use a general purpose dense\n",
    "solver, although it is better to use one that is based on a Cholesky factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, df, alpha, beta):\n",
    "    eta=1e-5 # Stopping criterion\n",
    "    \n",
    "    while df(x) < eta:\n",
    "        # 1. Determine a descent direction ∆x\n",
    "        deltax = df(x)\n",
    "        # 2. Line search: choose a step size t > 0\n",
    "        while f(x + t*deltax) > f(x) + alpha*t*df(x)*deltax:\n",
    "                t = beta*t\n",
    "        # 3. Update: x := x + t∆x\n",
    "        x = x + t*df(x)\n",
    "        \n",
    "    return x, f(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(f, df, ddf, alpha, beta):\n",
    "    epsilon = 1e-5\n",
    "    \n",
    "    lambd = 1e10\n",
    "    while lambd/2 < epsilon:\n",
    "        # 1. Determine a descent direction ∆x\n",
    "        deltax = −inverse(ddf(x))*df(x)\n",
    "        lambd = -df(x)*deltax\n",
    "        # 2. Line search: choose a step size t > 0\n",
    "        while f(x + t*deltax) > f(x) + alpha*t*df(x)*deltax:\n",
    "                t = beta*t\n",
    "                \n",
    "        # 3. Update: x := x + t∆x\n",
    "        x = x + t*df(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a74a1",
   "metadata": {},
   "source": [
    "# 10.15 Equality constrained entropy maximization.\n",
    "\n",
    "$$\n",
    "\\min ~f(x) = \\sum_{i=1}^{n} x_i log(x_i)\n",
    "$$\n",
    "$$\n",
    "subject~to~Ax=b\n",
    "$$\n",
    "\n",
    "(a) Standard Newton method. You can use initial point x (0) = x̂.\n",
    "\n",
    "(b) Infeasible start Newton method. You can use initial point x (0) = x̂ (to compare with\n",
    "the standard Newton method), and also the initial point x (0) = 1.\n",
    "\n",
    "(c) Dual Newton method, i.e., the standard Newton method applied to the dual problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe270ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inv = np.linalg.inverse\n",
    "\n",
    "\n",
    "def eqc_newton(f, df, ddf, A, alpha, beta)\n",
    "\n",
    "\n",
    "    w = inv(A.T)@inv(ddf(x))@df(x)\n",
    "    deltax = −inv(ddf(x))@(df(x)-A.T@w)\n",
    "    l(x) = np.sqrt(delta_x.T@ddf(x)@delta_x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "r(x, v) = (r_dual(x, v), r_pri(x, v))\n",
    "\n",
    "r_dual = df(x) + A*v\n",
    "r_pri = A*b\n",
    "\n",
    "\n",
    "\n",
    "def infeasible_newton_method(f, df, ddf, alpha, beta):\n",
    "    epsilon = 1e-5\n",
    "    v=0\n",
    "    lambd = 1e10\n",
    "    while lambd/2 < epsilon:\n",
    "        \n",
    "        \n",
    "        \n",
    "        sol = -[diag(1./x) A’; A zeros(p,p)] / r\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 1. Determine a descent direction ∆x\n",
    "        deltax = −inverse(ddf(x))*df(x)\n",
    "        lambd = -df(x)*deltax\n",
    "        # 2. Line search: choose a step size t > 0\n",
    "        while f(x + t*deltax) > f(x) + alpha*t*df(x)*deltax:\n",
    "                t = beta*t\n",
    "                v=v+αΔv\n",
    "                \n",
    "        # 3. Update: x := x + t∆x\n",
    "        x = x + t*df(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
