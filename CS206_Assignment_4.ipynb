{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466cd22c",
   "metadata": {},
   "source": [
    "# 9.30 Gradient and Newton methods\n",
    "\n",
    "minimize\n",
    "f (x) = −\n",
    "P m\n",
    "i=1\n",
    "log(1 − a Ti x) −\n",
    "P n\n",
    "i=1\n",
    "log(1 − x 2 i ),\n",
    "\n",
    "a) Use the gradient method to solve the problem, using reasonable choices for the back-\n",
    "tracking parameters, and a stopping criterion of the form k∇f (x)k 2 ≤ η. Plot the\n",
    "objective function and step length versus iteration number. (Once you have deter-\n",
    "mined p ⋆ to high accuracy, you can also plot f − p ⋆ versus iteration.) Experiment\n",
    "with the backtracking parameters α and β to see their effect on the total number of\n",
    "iterations required. Carry these experiments out for several instances of the problem,\n",
    "of different sizes.\n",
    "\n",
    "(b) Repeat using Newton’s method, with stopping criterion based on the Newton decre-\n",
    "ment λ 2 . Look for quadratic convergence. You do not have to use an efficient method\n",
    "to compute the Newton step, as in exercise 9.27; you can use a general purpose dense\n",
    "solver, although it is better to use one that is based on a Cholesky factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, df, alpha, beta):\n",
    "    eta=1e-5 # Stopping criterion\n",
    "    \n",
    "    while df(x) < eta:\n",
    "        # 1. Determine a descent direction ∆x\n",
    "        deltax = df(x)\n",
    "        # 2. Line search: choose a step size t > 0\n",
    "        while f(x + t*deltax) > f(x) + alpha*t*df(x)*deltax:\n",
    "                t = beta*t\n",
    "        # 3. Update: x := x + t∆x\n",
    "        x = x + t*df(x)\n",
    "        \n",
    "    return x, f(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(f, df, ddf, alpha, beta):\n",
    "    epsilon = 1e-5\n",
    "    \n",
    "    lambd = 1e10\n",
    "    while lambd/2 < epsilon:\n",
    "        # 1. Determine a descent direction ∆x\n",
    "        deltax = −inverse(ddf(x))*df(x)\n",
    "        lambd = -df(x)*deltax\n",
    "        # 2. Line search: choose a step size t > 0\n",
    "        while f(x + t*deltax) > f(x) + alpha*t*df(x)*deltax:\n",
    "                t = beta*t\n",
    "                \n",
    "        # 3. Update: x := x + t∆x\n",
    "        x = x + t*df(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a74a1",
   "metadata": {},
   "source": [
    "# 10.15 Equality constrained entropy maximization.\n",
    "\n",
    "$$\n",
    "\\min ~f(x) = \\sum_{i=1}^{n} x_i log(x_i)\n",
    "$$\n",
    "$$\n",
    "subject~to~Ax=b\n",
    "$$\n",
    "\n",
    "(a) Standard Newton method. You can use initial point x (0) = x̂.\n",
    "\n",
    "(b) Infeasible start Newton method. You can use initial point x (0) = x̂ (to compare with\n",
    "the standard Newton method), and also the initial point x (0) = 1.\n",
    "\n",
    "(c) Dual Newton method, i.e., the standard Newton method applied to the dual problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12080f1",
   "metadata": {},
   "source": [
    "For the sake of symplicity we will calculate the entropy in nats and convert it to bits afterwards as that should not impact the final result\n",
    "\n",
    "$\\nabla f(x) = 1+ log(x_i)$\n",
    "\n",
    "$\\nabla^2 f(x) = 1/x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dfe270ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "1.4218793694222616\n",
      "#######################################\n",
      "0.5678699436053058\n",
      "#######################################\n",
      "0.2064459829913621\n",
      "#######################################\n",
      "0.04056265824167877\n",
      "#######################################\n",
      "0.0013656236841484773\n",
      "#######################################\n",
      "1.5890196074238183e-06\n",
      "#######################################\n",
      "2.3644503128889965e-12\n"
     ]
    }
   ],
   "source": [
    "#https://www.cs.cmu.edu/~ggordon/10725-F12/scribes/10725_Lecture12.pdf\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from math import log\n",
    "# inv = np.linalg.inv\n",
    "\n",
    "def f(x):\n",
    "    return sum([xi*log(xi) for xi in x])\n",
    "\n",
    "def df(x):\n",
    "    return np.array([1+log(xi) for xi in x])\n",
    "\n",
    "def ddf(x):\n",
    "    A = np.identity(len(x))\n",
    "    for i in range(len(x)):\n",
    "        A[i,i] = 1/x[i]\n",
    "    return A\n",
    "\n",
    "\n",
    "def eqc_newton(f, df, ddf, A, alpha, beta, x0):\n",
    "    lambd = 1e10\n",
    "    epsilon = 1e-7\n",
    "    x = x0\n",
    "    \n",
    "    for i in range(100):\n",
    "        #1. Compute the Newton step and decrement ∆x nt , λ(x).\n",
    "        print(\"#######################################\")\n",
    "        upper = np.concatenate([ddf(x), A.T], axis=1)\n",
    "        lower = np.concatenate([A, np.zeros((A.shape[0], A.shape[0]))], axis=1)\n",
    "        \n",
    "        matrix = np.concatenate([upper, lower], axis =0)\n",
    "        vector = np.append(-df(x), np.zeros(A.shape[0]))\n",
    "        \n",
    "        result = inv(matrix)@vector\n",
    "        \n",
    "        delta_x = result[0:len(x)]\n",
    "\n",
    "        lambd = np.sqrt(delta_x.T@ddf(x)@delta_x)\n",
    "        print(lambd)\n",
    "        \n",
    "        #2. Stopping criterion. quit if λ 2 /2 ≤ ǫ.\n",
    "        if lambd/2 < epsilon:\n",
    "            break\n",
    "        \n",
    "        #3. Line search. Choose step size t by backtracking line search.\n",
    "        t = 1.0\n",
    "        \n",
    "#         #3.1 Guarantee x in dom f(x)\n",
    "#         while min(x+t*delta_x) <= 0:\n",
    "#             t = beta*t\n",
    "        \n",
    "        \n",
    "        #3.1 Line search x in dom f(x)\n",
    "        while f(x + t*delta_x) > f(x) + alpha*t*df(x).T@delta_x:\n",
    "                t = beta*t\n",
    "        \n",
    "        #4. Update. x := x + t∆x nt .\n",
    "        x = x + t*delta_x\n",
    "        \n",
    "        \n",
    "    return\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#n = 100 and p = 30 by choosing A randomly (checking that it has full rank)\n",
    "n = 100\n",
    "p = 30\n",
    "A = np.random.rand(p, n)\n",
    "x0 = np.abs(np.random.rand(n))\n",
    "x0 = x0/np.linalg.norm(x0)\n",
    "b = A@x0\n",
    "assert np.linalg.matrix_rank(A) == p\n",
    "alpha = 0.2\n",
    "beta = 0.5\n",
    "\n",
    "\n",
    "eqc_newton(f, df, ddf, A, alpha, beta, x0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f2d66c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "0.0 251.87500528209054\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 't' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [125], line 71\u001b[0m\n\u001b[1;32m     67\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m     68\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[43meqc_infeasible_newton\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv0\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn [125], line 38\u001b[0m, in \u001b[0;36meqc_infeasible_newton\u001b[0;34m(f, df, ddf, A, b, alpha, beta, x0, v0)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#3.1 Guarantee x in dom f(x)\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(x\u001b[38;5;241m+\u001b[39m\u001b[43mt\u001b[49m\u001b[38;5;241m*\u001b[39mdelta_x) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     39\u001b[0m     t \u001b[38;5;241m=\u001b[39m beta\u001b[38;5;241m*\u001b[39mt\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#3. Line search. Choose step size t by backtracking line search.\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 't' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def eqc_infeasible_newton(f, df, ddf, A, b, alpha, beta, x0, v0):\n",
    "    lambd = 1e10\n",
    "    epsilon = 1e-7\n",
    "    x = x0\n",
    "    v = v0\n",
    "    \n",
    "    for i in range(100):\n",
    "        #1. Compute the Newton step and decrement ∆x nt , λ(x).\n",
    "        print(\"#######################################\")\n",
    "        upper = np.concatenate([ddf(x), A.T], axis=1)\n",
    "        lower = np.concatenate([A, np.zeros((A.shape[0], A.shape[0]))], axis=1)\n",
    "        \n",
    "        matrix = np.concatenate([upper, lower], axis =0)\n",
    "        vector = -np.append(df(x), A@x-b)\n",
    "        \n",
    "        result = inv(matrix)@vector\n",
    "        \n",
    "        delta_x = result[0:len(x)]\n",
    "        w = result[len(x):]\n",
    "        delta_v = w - v\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        def r(x, v):\n",
    "            primal = df(x) + A.T@v,\n",
    "            dual = A@x-b\n",
    "            return np.append(primal, dual)\n",
    "        \n",
    "        \n",
    "        #2. Stopping criterion\n",
    "        print(f(x), np.linalg.norm(r(x, v)))\n",
    "        if sum(A@x - b) <= epsilon and np.linalg.norm(r(x, v)) <= epsilon:\n",
    "            break\n",
    "            \n",
    "                \n",
    "\n",
    "        \n",
    "        #3. Line search. Choose step size t by backtracking line search.\n",
    "        t = 1.0\n",
    "#3.1 Guarantee x in dom f(x)\n",
    "        while min(x+t*delta_x) <= 0:\n",
    "            t = beta*t\n",
    "        \n",
    "        \n",
    "        while np.linalg.norm(r(x+t*delta_x, v+t*delta_v)) < (1-alpha)*t*np.linalg.norm(r(x, v)):   \n",
    "            t = beta*t\n",
    "        \n",
    "        #4. Update. x := x + t∆x nt .\n",
    "        x = x + t*delta_x\n",
    "        v = v+ t*delta_v\n",
    "        \n",
    "        \n",
    "    return\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#n = 100 and p = 30 by choosing A randomly (checking that it has full rank)\n",
    "n = 100\n",
    "p = 30\n",
    "A = np.random.rand(p, n)\n",
    "x0 = np.abs(np.random.rand(n))\n",
    "x0 = x0/np.linalg.norm(x0)\n",
    "b = A@x0\n",
    "x0 = np.ones(n)\n",
    "v0 = np.zeros(p)\n",
    "assert np.linalg.matrix_rank(A) == p\n",
    "alpha = 0.2\n",
    "beta = 0.5\n",
    "\n",
    "\n",
    "eqc_infeasible_newton(f, df, ddf, A, b, alpha, beta, x0, v0) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
