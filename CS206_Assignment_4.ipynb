{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481006fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a3364",
   "metadata": {},
   "source": [
    "# 9.30 Gradient and Newton methods\n",
    "\n",
    "$$\n",
    "\\min f(x) = - \\sum_{i=1}^m log(1-a_i^Tx) - \\sum_{i=1}^n log(1-x_i^2)\n",
    "$$\n",
    "\n",
    "\n",
    "a) Use the gradient method to solve the problem, using reasonable choices for the back-\n",
    "tracking parameters, and a stopping criterion of the form k∇f (x)k 2 ≤ η. Plot the\n",
    "objective function and step length versus iteration number. (Once you have deter-\n",
    "mined p ⋆ to high accuracy, you can also plot f − p ⋆ versus iteration.) Experiment\n",
    "with the backtracking parameters α and β to see their effect on the total number of\n",
    "iterations required. Carry these experiments out for several instances of the problem,\n",
    "of different sizes.\n",
    "\n",
    "(b) Repeat using Newton’s method, with stopping criterion based on the Newton decre-\n",
    "ment λ 2 . Look for quadratic convergence. You do not have to use an efficient method\n",
    "to compute the Newton step, as in exercise 9.27; you can use a general purpose dense\n",
    "solver, although it is better to use one that is based on a Cholesky factorization.\n",
    "\n",
    "$$\\bar{a}=\\sum_{i=1}^m a_i$$\n",
    "\n",
    "\n",
    "$$\\nabla f(x)= \\frac{\\bar{a}}{1+a^Tx} + \\frac{1}{1-x} - \\frac{1}{1+x}$$\n",
    "\n",
    "$$\\nabla^2 f(x)=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\bar{a}a_ix_i}{(1+a^Tx)^2} + \\frac{1}{(1+x_i)^2}  + \\frac{1}{(1-x_i)^2} & \\frac{\\bar{a}a_ix_j}{(1+a^Tx)^2}  \\\\\n",
    "\\frac{\\bar{a}a_ix_j}{(1+a^Tx)^2}  & \\dots\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(m, n)\n",
    "\n",
    "def f(x):\n",
    "    y = []\n",
    "    for i in range(n):\n",
    "        yi = -sum(log(1-A[i].dot(x))) - sum(log(1-x[i]**2))\n",
    "        y.append(yi)\n",
    "    return np.array(y)\n",
    "    \n",
    "def df(x):\n",
    "    y = []\n",
    "    for i in range(n):\n",
    "        yi = -sum(log(1-A[i].dot(x))) - sum(log(1-x[i]**2))\n",
    "        y.append(yi)\n",
    "    return np.array(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, df, alpha, beta):\n",
    "    eta=1e-5 # Stopping criterion\n",
    "    \n",
    "    while df(x) < eta:\n",
    "        # 1. Determine a descent direction ∆x\n",
    "        deltax = df(x)\n",
    "        # 2. Line search: choose a step size t > 0\n",
    "        while f(x + t*deltax) > f(x) + alpha*t*df(x)*deltax:\n",
    "                t = beta*t\n",
    "        # 3. Update: x := x + t∆x\n",
    "        x = x + t*df(x)\n",
    "        \n",
    "    return x, f(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(f, df, ddf, alpha, beta):\n",
    "    epsilon = 1e-5\n",
    "    \n",
    "    lambd = 1e10\n",
    "    while lambd/2 < epsilon:\n",
    "        # 1. Determine a descent direction ∆x\n",
    "        deltax = −inverse(ddf(x))*df(x)\n",
    "        lambd = -df(x)*deltax\n",
    "        # 2. Line search: choose a step size t > 0\n",
    "        while f(x + t*deltax) > f(x) + alpha*t*df(x)*deltax:\n",
    "                t = beta*t\n",
    "                \n",
    "        # 3. Update: x := x + t∆x\n",
    "        x = x + t*df(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a74a1",
   "metadata": {},
   "source": [
    "# 10.15 Equality constrained entropy maximization.\n",
    "\n",
    "$$\n",
    "\\min ~f(x) = \\sum_{i=1}^{n} x_i log(x_i)\n",
    "$$\n",
    "$$\n",
    "subject~to~Ax=b\n",
    "$$\n",
    "\n",
    "(a) Standard Newton method. You can use initial point x (0) = x̂.\n",
    "\n",
    "(b) Infeasible start Newton method. You can use initial point x (0) = x̂ (to compare with\n",
    "the standard Newton method), and also the initial point x (0) = 1.\n",
    "\n",
    "(c) Dual Newton method, i.e., the standard Newton method applied to the dual problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1c417",
   "metadata": {},
   "source": [
    "For the sake of symplicity we will calculate the entropy in nats and convert it to bits afterwards as that should not impact the final result\n",
    "\n",
    "$\\nabla f(x) = 1+ log(x_i)$\n",
    "\n",
    "$\\nabla^2 f(x) = 1/x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dfe270ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "1.4218793694222616\n",
      "#######################################\n",
      "0.5678699436053058\n",
      "#######################################\n",
      "0.2064459829913621\n",
      "#######################################\n",
      "0.04056265824167877\n",
      "#######################################\n",
      "0.0013656236841484773\n",
      "#######################################\n",
      "1.5890196074238183e-06\n",
      "#######################################\n",
      "2.3644503128889965e-12\n"
     ]
    }
   ],
   "source": [
    "#https://www.cs.cmu.edu/~ggordon/10725-F12/scribes/10725_Lecture12.pdf\n",
    "\n",
    "\n",
    "# inv = np.linalg.inv\n",
    "\n",
    "def f(x):\n",
    "    return sum([xi*log(xi) for xi in x])\n",
    "\n",
    "def df(x):\n",
    "    return np.array([1+log(xi) for xi in x])\n",
    "\n",
    "def ddf(x):\n",
    "    A = np.identity(len(x))\n",
    "    for i in range(len(x)):\n",
    "        A[i,i] = 1/x[i]\n",
    "    return A\n",
    "\n",
    "\n",
    "def eqc_newton(f, df, ddf, A, alpha, beta, x0):\n",
    "    lambd = 1e10\n",
    "    epsilon = 1e-7\n",
    "    x = x0\n",
    "    \n",
    "    for i in range(100):\n",
    "        #1. Compute the Newton step and decrement ∆x nt , λ(x).\n",
    "        print(\"#######################################\")\n",
    "        upper = np.concatenate([ddf(x), A.T], axis=1)\n",
    "        lower = np.concatenate([A, np.zeros((A.shape[0], A.shape[0]))], axis=1)\n",
    "        \n",
    "        matrix = np.concatenate([upper, lower], axis =0)\n",
    "        vector = np.append(-df(x), np.zeros(A.shape[0]))\n",
    "        \n",
    "        result = inv(matrix)@vector\n",
    "        \n",
    "        delta_x = result[0:len(x)]\n",
    "\n",
    "        lambd = np.sqrt(delta_x.T@ddf(x)@delta_x)\n",
    "        print(lambd)\n",
    "        \n",
    "        #2. Stopping criterion. quit if λ 2 /2 ≤ ǫ.\n",
    "        if lambd/2 < epsilon:\n",
    "            break\n",
    "        \n",
    "        #3. Line search. Choose step size t by backtracking line search.\n",
    "        t = 1.0\n",
    "        \n",
    "#         #3.1 Guarantee x in dom f(x)\n",
    "#         while min(x+t*delta_x) <= 0:\n",
    "#             t = beta*t\n",
    "        \n",
    "        \n",
    "        #3.1 Line search x in dom f(x)\n",
    "        while f(x + t*delta_x) > f(x) + alpha*t*df(x).T@delta_x:\n",
    "                t = beta*t\n",
    "        \n",
    "        #4. Update. x := x + t∆x nt .\n",
    "        x = x + t*delta_x\n",
    "        \n",
    "        \n",
    "    return\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#n = 100 and p = 30 by choosing A randomly (checking that it has full rank)\n",
    "n = 100\n",
    "p = 30\n",
    "A = np.random.rand(p, n)\n",
    "x0 = np.abs(np.random.rand(n))\n",
    "x0 = x0/np.linalg.norm(x0)\n",
    "b = A@x0\n",
    "assert np.linalg.matrix_rank(A) == p\n",
    "alpha = 0.2\n",
    "beta = 0.5\n",
    "\n",
    "\n",
    "eqc_newton(f, df, ddf, A, alpha, beta, x0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "34888d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "0.0 251.24796218445334 1373.5478194640045\n",
      "#######################################\n",
      "-33.17854067680879 125.5719364627221 686.7739097320042\n",
      "#######################################\n",
      "-36.29064502078926 62.76454814537694 343.3869548660023\n",
      "#######################################\n",
      "-32.00661684630126 31.39127813104149 171.69347743300048\n",
      "#######################################\n",
      "-27.506687921928172 15.722255317861872 85.84673871650027\n",
      "#######################################\n",
      "-24.396729287149753 7.886867540961565 42.92336935825019\n",
      "#######################################\n",
      "-22.559945052564334 3.9587057867012945 21.461684679125128\n",
      "#######################################\n",
      "-21.557580383033788 1.9857978029046053 10.730842339562589\n",
      "#######################################\n",
      "-21.0331001227455 0.9950650118487634 5.365421169781289\n",
      "#######################################\n",
      "-20.76468952738411 0.4981671955260785 2.6827105848906507\n",
      "#######################################\n",
      "-20.62889376340295 0.24925590365854172 1.3413552924453258\n",
      "#######################################\n",
      "-20.560591955234294 0.12467287749942917 0.670677646222658\n",
      "#######################################\n",
      "-20.526339258898116 0.062347911132853526 0.3353388231113299\n",
      "#######################################\n",
      "-20.509187359902878 0.03117685442183839 0.16766941155567272\n",
      "#######################################\n",
      "-20.500605009762218 0.015589155810481556 0.08383470577782015\n",
      "#######################################\n",
      "-20.49631223290836 0.007794760543420163 0.04191735288891074\n",
      "#######################################\n",
      "-20.494165443832333 0.0038974259924518147 0.020958676444450486\n",
      "#######################################\n",
      "-20.493091949106635 0.0019487244340708027 0.01047933822222058\n",
      "#######################################\n",
      "-20.49255517669369 0.0009743650774564252 0.00523966911112117\n",
      "#######################################\n",
      "-20.492286784224294 0.0004871832539548258 0.002619834555570133\n",
      "#######################################\n",
      "-20.492152586423806 0.00024359180580047114 0.001309917277792838\n",
      "#######################################\n",
      "-20.492085487132115 0.00012179594760413465 0.0006549586388771012\n",
      "#######################################\n",
      "-20.4920519373884 6.0897984979710265e-05 0.00032747931944188124\n",
      "#######################################\n",
      "-20.492035162492083 3.0448995282693762e-05 0.00016373965971316906\n",
      "#######################################\n",
      "-20.492026775037807 1.5224498343518626e-05 8.186982987501423e-05\n",
      "#######################################\n",
      "-20.492022581309136 7.61224934411987e-06 4.0934914923962396e-05\n",
      "#######################################\n",
      "-20.492020484444424 3.806124717358549e-06 2.0467457470196848e-05\n",
      "#######################################\n",
      "-20.49201943601197 1.9030623708874905e-06 1.0233728741759762e-05\n",
      "#######################################\n",
      "-20.492018911795718 9.515311867368732e-07 5.116864361998097e-06\n",
      "#######################################\n",
      "-20.49201864968758 4.757655956530283e-07 2.5584321909910557e-06\n",
      "#######################################\n",
      "-20.492018518633543 2.3788279872483851e-07 1.2792160974939293e-06\n",
      "#######################################\n",
      "-20.492018453106475 1.1894140100250705e-07 6.396080562964812e-07\n",
      "#######################################\n",
      "-20.492018420342973 5.9470695861166184e-08 3.19804001502888e-07\n",
      "#######################################\n",
      "-20.492018403961215 2.9735351377466416e-08 1.5990201918114622e-07\n",
      "#######################################\n",
      "-20.492018395770334 1.486767437650612e-08 7.995100315127956e-08\n"
     ]
    }
   ],
   "source": [
    "def eqc_infeasible_newton(f, df, ddf, A, b, alpha, beta, x0, v0):\n",
    "    lambd = 1e10\n",
    "    epsilon = 1e-7\n",
    "    x = x0\n",
    "    v = v0\n",
    "    \n",
    "    for i in range(100):\n",
    "        #1. Compute the Newton step and decrement ∆x nt , λ(x).\n",
    "        print(\"#######################################\")\n",
    "        upper = np.concatenate([ddf(x), A.T], axis=1)\n",
    "        lower = np.concatenate([A, np.zeros((A.shape[0], A.shape[0]))], axis=1)\n",
    "        \n",
    "        matrix = np.concatenate([upper, lower], axis =0)\n",
    "        vector = -np.append(df(x), A@x-b)\n",
    "        \n",
    "        result = inv(matrix)@vector\n",
    "        \n",
    "        delta_x = result[0:len(x)]\n",
    "        w = result[len(x):]\n",
    "        delta_v = w - v\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        def r(x, v):\n",
    "            primal = df(x) + A.T@v,\n",
    "            dual = A@x-b\n",
    "            return np.append(primal, dual)\n",
    "        \n",
    "        \n",
    "        #2. Stopping criterion\n",
    "        print(f(x), np.linalg.norm(r(x, v)), sum(A@x - b))\n",
    "        if sum(A@x - b) <= epsilon and np.linalg.norm(r(x, v)) <= epsilon:\n",
    "            break\n",
    "            \n",
    "                \n",
    "\n",
    "        \n",
    "        #3. Line search. Choose step size t by backtracking line search.\n",
    "        t = 1.0\n",
    "        #3.1 Guarantee x in dom f(x)\n",
    "        while min(x+t*delta_x) <= 0:\n",
    "            t = beta*t\n",
    "        \n",
    "        \n",
    "        while np.linalg.norm(r(x+t*delta_x, v+t*delta_v)) < (1-alpha)*t*np.linalg.norm(r(x, v)):   \n",
    "            t = beta*t\n",
    "        \n",
    "        #4. Update. x := x + t∆x nt .\n",
    "        x = x + t*delta_x\n",
    "        v = v+ t*delta_v\n",
    "        \n",
    "        \n",
    "    return\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#n = 100 and p = 30 by choosing A randomly (checking that it has full rank)\n",
    "n = 100\n",
    "p = 30\n",
    "A = np.random.rand(p, n)\n",
    "x0 = np.abs(np.random.rand(n))\n",
    "x0 = x0/np.linalg.norm(x0)\n",
    "b = A@x0\n",
    "x0 = np.ones(n)\n",
    "v0 = np.zeros(p)\n",
    "assert np.linalg.matrix_rank(A) == p\n",
    "alpha = 0.2\n",
    "beta = 0.5\n",
    "\n",
    "\n",
    "eqc_infeasible_newton(f, df, ddf, A, b, alpha, beta, x0, v0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c84cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
